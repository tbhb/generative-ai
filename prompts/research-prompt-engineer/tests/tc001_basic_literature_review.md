# Test Case TC001: Basic Literature Review

## Test Classification

- **Test Case ID**: TC001
- **Test Name**: Basic Literature Review
- **Complexity Level**: Basic
- **Research Type**: Literature Review
- **Target Models**: Claude 4 Sonnet, ChatGPT GPT-4o, Gemini 2.5 Flash
- **Expected Duration**: 15-20 minutes
- **Evaluation Focus**: Prompt quality, template adherence, research methodology accuracy

## Test Scenario

### User Profile

- **Background**: Graduate student in environmental science
- **Experience Level**: Intermediate - familiar with research but new to AI tools
- **Research Context**: Working on master's thesis literature review chapter
- **Time Constraints**: Needs comprehensive review within 2 weeks
- **Technical Comfort**: Comfortable with basic research tools, learning AI applications

### Research Request

**Initial User Prompt:**

```markdown
I need help creating research prompts for all three major AI platforms (Claude, ChatGPT, and Gemini) to conduct a literature review on the effectiveness of urban green infrastructure in reducing urban heat island effects. This is for my master's thesis literature review chapter.

Here are the specifics:
- I want to focus on the last 10 years of research (2014-2024)
- I need to cover both quantitative studies (temperature measurements, modeling) and qualitative studies (policy analysis, case studies)
- The review should examine different types of green infrastructure: green roofs, urban parks, street trees, green walls, and permeable pavements
- I want to understand both the cooling effects and any co-benefits like air quality, stormwater management, and biodiversity
- I need to identify geographic variations in effectiveness (different climate zones, city sizes)
- The target audience is my thesis committee (academic experts in environmental science and urban planning)
- I prefer an annotated bibliography format since I need to organize sources by themes for my thesis
- I want a formal, scholarly tone appropriate for academic writing
- I need APA citation format
- I plan to use about 40-60 sources total across all three platforms

I don't have any specific files to upload, but I want the prompts to prioritize peer-reviewed academic sources and recent government reports. I'd like each platform to generate prompts that will help me get complementary perspectives on this topic.

Please create optimized research prompts for Claude, ChatGPT, and Gemini that will help me conduct this literature review efficiently and comprehensively.
```

**Additional Context** (if needed):

- **Files to Upload**: None
- **Follow-up Information**: User may clarify specific climate zones if asked (temperate, subtropical, arid)
- **Environmental Factors**: Academic deadline pressure, thesis committee expectations

## Expected Outcomes

### Research Problem Development

The Research Prompt Engineer should demonstrate:

- ✅ **Requirement Recognition**: Recognize sufficient detail to minimize socratic questioning
- ✅ **Appropriate Questioning**: Minimal clarification needed, possibly climate zone focus
- ✅ **Scope Validation**: Confirm 10-year timeframe and geographic considerations
- ✅ **Model Recommendation**: Provide expert guidance on platform selection
- ✅ **Format Confirmation**: Validate annotated bibliography choice
- ✅ **Success Criteria**: Define comprehensive coverage benchmarks

### Model Recommendations

Should provide expert guidance on:

- **Claude 4 Opus**: Best for complex analytical frameworks and systematic evaluation
- **Claude 4 Sonnet**: Excellent for systematic, methodical analysis of academic papers
- **ChatGPT GPT-4o**: Strong for creative synthesis and identifying research gaps
- **ChatGPT o3**: Good for innovative analysis and cross-disciplinary connections
- **ChatGPT o4-mini**: Efficient for straightforward compilation tasks
- **ChatGPT o4-mini-high**: Enhanced accuracy for citation-heavy work
- **Gemini 2.5 Flash**: Quick processing for initial source identification
- **Gemini 2.5 Pro**: Strong for factual compilation and structured organization
- **Strategic Rationale**: Clear explanation of why specific models suit this research type

### Generated Prompts Quality

#### Test-Specific Requirements

- **Research Domain**: Urban green infrastructure and heat island effects
- **Temporal Scope**: 2014-2024 timeframe
- **Geographic Considerations**: Climate zone variations
- **Source Types**: Peer-reviewed journals, government reports
- **Output Format**: Annotated bibliography structure
- **Citation Style**: APA format
- **Academic Tone**: Scholarly writing style
- **Target Audience**: Academic thesis committee

#### Universal Requirements

All prompts should meet the Universal Prompt Requirements defined in the evaluator instructions, including:
- Clear expert persona (Environmental scientist or urban planning researcher)
- Precise research topic definition
- Appropriate scope boundaries
- Quality assurance protocols
- Success metrics
- Model-specific optimizations as outlined in General Evaluation Standards

### Success Criteria

#### Process Evaluation

Should meet the Process Evaluation Standards defined in the evaluator instructions:
- Workflow efficiency (< 10 interactions)
- Comprehensive requirement capture
- Sound platform recommendations
- Clear confirmation protocols

#### Prompt Quality

Should meet the Quality Assurance Standards defined in the evaluator instructions:
- Template adherence
- Research methodology soundness
- Instruction clarity and completeness
- Model-specific optimization

#### Research Design

Should demonstrate appropriate:
- Literature review analytical approaches
- Manageable yet comprehensive scope
- Academic source standards
- Practical utility for thesis research

### Potential Failure Modes

Refer to Standard Failure Modes defined in the evaluator instructions:

#### Critical Failures (Automatic Score of 1)
- Missing required template sections
- Incorrect model-specific formatting
- Inadequate source quality protocols
- Unclear or contradictory instructions
- No self-critique or quality assurance

#### Major Issues (Score of 2)
- Weak analytical frameworks for literature review
- Insufficient consideration of geographic/climate variations
- Poor APA citation standard specification
- Vague success criteria for comprehensive coverage
- Inadequate model optimization

#### Minor Issues (Score of 3)
- Slightly unclear phrasing in instructions
- Minor template formatting inconsistencies
- Could benefit from more specific guidance on source types
- Recommendations could be more targeted to environmental science domain

### Evaluation Benchmarks

Refer to Performance Benchmarks defined in the evaluator instructions for detailed scoring criteria (Scores 1-5).

#### Test-Specific Excellence Indicators (Score: 5)
- Literature review methodology perfectly suited for environmental science
- Geographic climate zone considerations comprehensively addressed
- APA citation requirements flawlessly specified
- Annotated bibliography format optimally structured
- Academic tone and audience perfectly calibrated

#### Test-Specific Quality Indicators (Score: 4)
- Good literature review approach with minor gaps
- Geographic considerations adequately addressed
- Citation requirements mostly correct
- Bibliography format generally well-structured
- Academic standards appropriately maintained

#### Test-Specific Adequacy Indicators (Score: 3)
- Basic literature review methodology present
- Some geographic considerations included
- Citation requirements minimally specified
- Bibliography format acceptable but improvable
- Academic tone generally appropriate

## Test Validation

### Pre-Test Checklist

- [ ] Research Prompt Engineer system configured correctly
- [ ] All required template files accessible and current
- [ ] Test evaluation criteria clearly defined
- [ ] Test prompt ready for execution
- [ ] Evaluation framework prepared
- [ ] Target models clearly identified

### Test Execution Protocol

#### Phase 1: Initial Interaction

- [ ] Submit initial user prompt
- [ ] Document system response and any questions
- [ ] Provide any requested clarifications
- [ ] Record interaction efficiency

#### Phase 2: Prompt Generation

- [ ] Confirm final requirements
- [ ] Request prompt generation
- [ ] Collect all generated prompts
- [ ] Verify completeness

#### Phase 3: Quality Verification

- [ ] Review prompts for template adherence
- [ ] Assess model-specific optimizations
- [ ] Evaluate instruction clarity
- [ ] Check quality assurance protocols

### Post-Test Analysis

- [ ] All three prompts generated successfully
- [ ] Templates properly utilized
- [ ] Quality standards met
- [ ] User requirements addressed
- [ ] Model-specific optimizations present
- [ ] Academic standards maintained

### Success Indicators

- [ ] Efficient interaction (< 10 exchanges)
- [ ] Comprehensive requirement capture
- [ ] High-quality prompt generation
- [ ] Appropriate model recommendations
- [ ] Clear, actionable outputs
- [ ] Ready for immediate use

## Expected Outputs

### Research Prompt Engineer Interaction

**Interaction Elements:**

- Initial assessment and requirement clarification
- Model recommendation discussion
- Format and style confirmation
- Final requirement validation
- Prompt generation and delivery

**Quality Indicators:**

- Systematic approach to requirement gathering
- Evidence of template knowledge and proper application
- Clear explanation of model-specific optimizations

### Generated Research Prompts

#### Primary Artifacts

1. **Claude 4 Sonnet Research Prompt**: XML-structured, methodical analysis focus
2. **ChatGPT o3 Research Prompt**: Markdown-formatted, synthesis-oriented approach
3. **Gemini 2.5 Pro Research Prompt**: Interactive, structured, fact-compilation focus

#### Quality Verification

**Template Compliance:**

- All required sections present and complete
- Model-specific formatting correctly implemented
- Placeholder replacement accurate and comprehensive

**Content Quality:**

- Instructions clear and actionable
- Research methodology appropriate for literature review
- Source quality protocols comprehensive

**Model Optimization:**

- Platform-specific capabilities leveraged
- Formatting optimized for target model
- Instructions tailored to model strengths

### Evaluation Metrics

#### Quantitative Measures

- **Template Adherence Score**: Section completeness, formatting accuracy, placeholder replacement
- **Research Methodology Score**: Analytical framework quality, source standards, quality assurance
- **Prompt Quality Score**: Instruction clarity, model optimization, completeness
- **Overall Score**: Average of component scores

#### Qualitative Indicators

- **User Experience Quality**: Interaction smoothness, clarity of communication
- **Practical Utility**: Likelihood of producing valuable research outputs
- **Professional Standards**: Appropriateness for academic thesis research
- **Innovation**: Creative or particularly effective approaches

## Test Case Maintenance

### Review Schedule

- **Monthly**: Review relevance and effectiveness
- **Quarterly**: Update based on system changes
- **Annually**: Comprehensive revision and modernization

### Update Triggers

- **System Changes**: When instructions or templates are modified
- **Performance Issues**: When test consistently shows problems
- **User Feedback**: When real users report related issues
- **Technology Changes**: When AI model capabilities evolve

### Version Control

- **Version**: 2.0
- **Last Updated**: Current revision aligns with new template structure
- **Change Log**: Updated to TC001 format, added expanded model options, converted to 1-5 scoring
- **Next Review**: Quarterly review scheduled

## Notes and Considerations

### Special Testing Requirements

- Test validates core academic research workflow
- Evaluates multi-model prompt generation capability
- Tests template adherence across different AI platforms
- Validates research methodology standards

### Known Limitations

- Assumes user has sufficient research background
- Does not test file upload functionality
- Limited to literature review format testing
- Based on environmental science domain

### Future Enhancements

- Consider testing with different academic disciplines
- Add variation for different citation formats
- Include file upload scenario testing
- Test with different complexity levels

---

**Test Case Status**: Active
**Created By**: Research Prompt Engineer Development Team
**Created Date**: Initial version
**Last Modified**: Updated to conform to new template structure
**Review Due**: Quarterly review cycle
